{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module LayerNorm is treated as a zero-op.\n",
      "Warning: module Adapter is treated as a zero-op.\n",
      "Warning: module NonDynamicallyQuantizableLinear is treated as a zero-op.\n",
      "Warning: module QuickGELU is treated as a zero-op.\n",
      "Warning: module Identity is treated as a zero-op.\n",
      "Warning: module Block is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module STCrossTransformer is treated as a zero-op.\n",
      "STCrossTransformer(\n",
      "  103.67 M, 99.803% Params, 405.3 GMac, 99.995% MACs, \n",
      "  (clip_conv1): Conv2d(589.82 k, 0.568% Params, 1.85 GMac, 0.456% MACs, 3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "  (clip_ln_pre): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "  (blocks): ModuleList(\n",
      "    (0-11): 12 x Block(\n",
      "      8.56 M, 8.245% Params, 33.62 GMac, 8.295% MACs, \n",
      "      (act): GELU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, approximate='none')\n",
      "      (clip_ln_1): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (WT_Adapter): Adapter(\n",
      "        295.87 k, 0.285% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (act): GELU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, approximate='none')\n",
      "        (D_fc1): Linear(147.65 k, 0.142% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=192, bias=True)\n",
      "        (D_fc2): Linear(148.22 k, 0.143% Params, 0.0 Mac, 0.000% MACs, in_features=192, out_features=768, bias=True)\n",
      "      )\n",
      "      (HT_Adapter): Adapter(\n",
      "        295.87 k, 0.285% Params, 933.19 MMac, 0.230% MACs, \n",
      "        (act): GELU(0, 0.000% Params, 605.18 KMac, 0.000% MACs, approximate='none')\n",
      "        (D_fc1): Linear(147.65 k, 0.142% Params, 465.39 MMac, 0.115% MACs, in_features=768, out_features=192, bias=True)\n",
      "        (D_fc2): Linear(148.22 k, 0.143% Params, 467.2 MMac, 0.115% MACs, in_features=192, out_features=768, bias=True)\n",
      "      )\n",
      "      (CLS_Adapter): Adapter(\n",
      "        295.87 k, 0.285% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (act): GELU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, approximate='none')\n",
      "        (D_fc1): Linear(147.65 k, 0.142% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=192, bias=True)\n",
      "        (D_fc2): Linear(148.22 k, 0.143% Params, 0.0 Mac, 0.000% MACs, in_features=192, out_features=768, bias=True)\n",
      "      )\n",
      "      (clip_attn): MultiheadAttention(\n",
      "        2.36 M, 2.274% Params, 15.94 GMac, 3.932% MACs, \n",
      "        (out_proj): NonDynamicallyQuantizableLinear(0, 0.000% Params, 0.0 Mac, 0.000% MACs, in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (S_Adapter): Adapter(\n",
      "        295.87 k, 0.285% Params, 933.19 MMac, 0.230% MACs, \n",
      "        (act): GELU(0, 0.000% Params, 605.18 KMac, 0.000% MACs, approximate='none')\n",
      "        (D_fc1): Linear(147.65 k, 0.142% Params, 465.39 MMac, 0.115% MACs, in_features=768, out_features=192, bias=True)\n",
      "        (D_fc2): Linear(148.22 k, 0.143% Params, 467.2 MMac, 0.115% MACs, in_features=192, out_features=768, bias=True)\n",
      "      )\n",
      "      (clip_ln_2): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "      (clip_mlp): Sequential(\n",
      "        4.72 M, 4.547% Params, 14.89 GMac, 3.672% MACs, \n",
      "        (c_fc): Linear(2.36 M, 2.274% Params, 7.45 GMac, 1.837% MACs, in_features=768, out_features=3072, bias=True)\n",
      "        (gelu): QuickGELU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "        (c_proj): Linear(2.36 M, 2.272% Params, 7.44 GMac, 1.835% MACs, in_features=3072, out_features=768, bias=True)\n",
      "      )\n",
      "      (S_MLP_Adapter): Adapter(\n",
      "        295.87 k, 0.285% Params, 933.19 MMac, 0.230% MACs, \n",
      "        (act): GELU(0, 0.000% Params, 605.18 KMac, 0.000% MACs, approximate='none')\n",
      "        (D_fc1): Linear(147.65 k, 0.142% Params, 465.39 MMac, 0.115% MACs, in_features=768, out_features=192, bias=True)\n",
      "        (D_fc2): Linear(148.22 k, 0.143% Params, 467.2 MMac, 0.115% MACs, in_features=192, out_features=768, bias=True)\n",
      "      )\n",
      "      (drop_path): Identity(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "    )\n",
      "  )\n",
      "  (clip_ln_post): LayerNorm(0, 0.000% Params, 0.0 Mac, 0.000% MACs, (768,), eps=1e-05, elementwise_affine=True)\n",
      "  (head_verb): Linear(74.59 k, 0.072% Params, 74.59 KMac, 0.000% MACs, in_features=768, out_features=97, bias=True)\n",
      "  (head_verb_dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      "  (head_noun): Linear(230.7 k, 0.222% Params, 230.7 KMac, 0.000% MACs, in_features=768, out_features=300, bias=True)\n",
      "  (head_noun_dropout): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.0, inplace=False)\n",
      ")\n",
      "rrr 405.33 GMac\n",
      "103.87 M\n",
      "Computational complexity:       405.33 GMac\n",
      "Number of parameters:           103.87 M\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "from timm.models import create_model\n",
    "import models.AIM\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    'aim_adapter_vit_base_patch16_224'\n",
    "    # pretrained=False,\n",
    "    # num_classes=300,\n",
    "    # all_frames=16 * args.num_segments,\n",
    "    # tubelet_size=args.tubelet_size,\n",
    "    # drop_rate=0.3,\n",
    "    # drop_path_rate=args.drop_path,\n",
    "    # attn_drop_rate=args.attn_drop_rate,\n",
    "    # drop_block_rate=None,\n",
    "    # use_mean_pooling=args.use_mean_pooling,\n",
    "    # init_scale=args.init_scale,\n",
    "    # fusion_method=args.fusion_method,\n",
    "#   head_drop_rate=args.head_drop\n",
    ")\n",
    "# (3, 8, 568, 320)\n",
    "macs, params = get_model_complexity_info(model, (3, 16, 224, 224), as_strings=True,\n",
    "                                        print_per_layer_stat=True, verbose=True)\n",
    "print('rrr',macs)\n",
    "print(params)\n",
    "print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aim_adapter_vit_base_patch16_224\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class argpar:\n",
    "    def __init__(self):\n",
    "        self.vmae_model = 'aim_adapter_vit_base_patch16_224'\n",
    "        self.nb_classes = '2'\n",
    "        \n",
    "        \n",
    "args = argpar()\n",
    "\n",
    "\n",
    "print(args.vmae_model)\n",
    "print(args.nb_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-12-26 15:05:40 3986445:3986445 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference         4.66%     609.153ms       100.00%       13.080s       13.080s             1  \n",
      "                                aten::linear         0.02%       2.355ms        74.24%        9.711s      66.514ms           146  \n",
      "                                 aten::addmm        69.95%        9.150s        74.20%        9.705s      66.472ms           146  \n",
      "                                 aten::copy_         8.68%        1.135s         8.68%        1.135s       3.569ms           318  \n",
      "          aten::scaled_dot_product_attention         0.00%      84.000us         7.48%     978.287ms      40.762ms            24  \n",
      "    aten::_scaled_dot_product_attention_math         0.27%      34.879ms         7.48%     978.203ms      40.758ms            24  \n",
      "                                 aten::clone         0.01%       1.216ms         4.46%     583.808ms       5.897ms            99  \n",
      "                                aten::matmul         0.01%       1.042ms         4.19%     547.925ms      11.415ms            48  \n",
      "                                   aten::bmm         4.17%     545.266ms         4.17%     545.266ms      11.360ms            48  \n",
      "                            aten::contiguous         0.00%     167.000us         3.71%     485.022ms       6.736ms            72  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.080s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-12-26 15:05:53 3986445:3986445 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-12-26 15:05:53 3986445:3986445 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from ptflops import get_model_complexity_info\n",
    "from timm.models import create_model\n",
    "import models.AIM\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(\n",
    "    'aim_adapter_vit_base_patch16_224'\n",
    "    # pretrained=False,\n",
    "    # num_classes=300,\n",
    "    # all_frames=16 * args.num_segments,\n",
    "    # tubelet_size=args.tubelet_size,\n",
    "    # drop_rate=0.3,\n",
    "    # drop_path_rate=args.drop_path,\n",
    "    # attn_drop_rate=args.attn_drop_rate,\n",
    "    # drop_block_rate=None,\n",
    "    # use_mean_pooling=args.use_mean_pooling,\n",
    "    # init_scale=args.init_scale,\n",
    "    # fusion_method=args.fusion_method,\n",
    "#   head_drop_rate=args.head_drop\n",
    ")\n",
    "# (3, 8, 568, 320)\n",
    "inputs = torch.rand(5, 3, 16, 224, 224)\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "        \n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "# macs, params = get_model_complexity_info(model, (3, 16, 224, 224), as_strings=True,\n",
    "#                                         print_per_layer_stat=True, verbose=True)\n",
    "# print('rrr',macs)\n",
    "# print(params)\n",
    "# print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "# print('{:<30}  {:<8}'.format('Number of parameters: ', params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             model_inference         4.66%     609.153ms       100.00%       13.080s       13.080s             1                                                                                []  \n",
      "                                 aten::addmm        23.59%        3.085s        25.03%        3.274s     136.413ms            24                                       [[2304], [15760, 768], [768, 2304], [], []]  \n",
      "                                aten::linear         0.00%     275.000us        16.81%        2.199s     183.277ms            12                                             [[197, 80, 768], [3072, 768], [3072]]  \n",
      "                                 aten::addmm        15.77%        2.063s        16.81%        2.199s     183.220ms            12                                       [[3072], [15760, 768], [768, 3072], [], []]  \n",
      "                                aten::linear         0.00%     286.000us        16.54%        2.164s     180.305ms            12                                             [[197, 80, 3072], [768, 3072], [768]]  \n",
      "                                 aten::addmm        16.30%        2.131s        16.54%        2.163s     180.244ms            12                                       [[768], [15760, 3072], [3072, 768], [], []]  \n",
      "                                aten::linear         0.00%     250.000us        12.55%        1.641s     136.781ms            12                                             [[16, 985, 768], [2304, 768], [2304]]  \n",
      "                                aten::linear         0.00%     265.000us        12.49%        1.634s     136.147ms            12                                             [[197, 80, 768], [2304, 768], [2304]]  \n",
      "                                aten::linear         0.00%     174.000us         8.68%        1.135s      47.306ms            24                                                 [[15760, 768], [768, 768], [768]]  \n",
      "                                 aten::addmm         8.14%        1.065s         8.68%        1.135s      47.285ms            24                                         [[768], [15760, 768], [768, 768], [], []]  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 13.080s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference         4.66%     609.153ms       100.00%       13.080s       13.080s             1  \n",
      "                               aten::permute         0.00%     570.000us         0.00%     644.000us      11.926us            54  \n",
      "                            aten::as_strided         0.01%     681.000us         0.01%     681.000us       0.988us           689  \n",
      "                               aten::reshape         0.01%       1.402ms         0.77%     100.997ms     668.854us           151  \n",
      "                                 aten::clone         0.01%       1.216ms         4.46%     583.808ms       5.897ms            99  \n",
      "                            aten::empty_like         0.01%     703.000us         0.02%       2.155ms      21.768us            99  \n",
      "                                 aten::empty         0.02%       2.046ms         0.02%       2.046ms       9.472us           216  \n",
      "                                 aten::copy_         8.68%        1.135s         8.68%        1.135s       3.569ms           318  \n",
      "                          aten::_unsafe_view         0.00%     594.000us         0.00%     594.000us       7.920us            75  \n",
      "                                aten::conv2d         0.00%       7.000us         0.44%      58.126ms      58.126ms             1  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.080s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference         4.66%     609.153ms       100.00%       13.080s       13.080s             1  \n",
      "                               aten::permute         0.00%     570.000us         0.00%     644.000us      11.926us            54  \n",
      "                            aten::as_strided         0.01%     681.000us         0.01%     681.000us       0.988us           689  \n",
      "                               aten::reshape         0.01%       1.402ms         0.77%     100.997ms     668.854us           151  \n",
      "                                 aten::clone         0.01%       1.216ms         4.46%     583.808ms       5.897ms            99  \n",
      "                            aten::empty_like         0.01%     703.000us         0.02%       2.155ms      21.768us            99  \n",
      "                                 aten::empty         0.02%       2.046ms         0.02%       2.046ms       9.472us           216  \n",
      "                                 aten::copy_         8.68%        1.135s         8.68%        1.135s       3.569ms           318  \n",
      "                          aten::_unsafe_view         0.00%     594.000us         0.00%     594.000us       7.920us            75  \n",
      "                                aten::conv2d         0.00%       7.000us         0.44%      58.126ms      58.126ms             1  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 13.080s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load CLIP ckpt from /data/datasets/Epickitchens100_clips/ViT-B-16.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "clip_finetune = '/data/datasets/Epickitchens100_clips/ViT-B-16.pt'\n",
    "clip_checkpoint = torch.jit.load(clip_finetune, map_location='cpu')\n",
    "\n",
    "print(\"Load CLIP ckpt from %s\" % clip_finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_clip = clip_checkpoint.visual.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['class_embedding', 'positional_embedding', 'proj', 'conv1.weight', 'ln_pre.weight', 'ln_pre.bias', 'transformer.resblocks.0.attn.in_proj_weight', 'transformer.resblocks.0.attn.in_proj_bias', 'transformer.resblocks.0.attn.out_proj.weight', 'transformer.resblocks.0.attn.out_proj.bias', 'transformer.resblocks.0.ln_1.weight', 'transformer.resblocks.0.ln_1.bias', 'transformer.resblocks.0.mlp.c_fc.weight', 'transformer.resblocks.0.mlp.c_fc.bias', 'transformer.resblocks.0.mlp.c_proj.weight', 'transformer.resblocks.0.mlp.c_proj.bias', 'transformer.resblocks.0.ln_2.weight', 'transformer.resblocks.0.ln_2.bias', 'transformer.resblocks.1.attn.in_proj_weight', 'transformer.resblocks.1.attn.in_proj_bias', 'transformer.resblocks.1.attn.out_proj.weight', 'transformer.resblocks.1.attn.out_proj.bias', 'transformer.resblocks.1.ln_1.weight', 'transformer.resblocks.1.ln_1.bias', 'transformer.resblocks.1.mlp.c_fc.weight', 'transformer.resblocks.1.mlp.c_fc.bias', 'transformer.resblocks.1.mlp.c_proj.weight', 'transformer.resblocks.1.mlp.c_proj.bias', 'transformer.resblocks.1.ln_2.weight', 'transformer.resblocks.1.ln_2.bias', 'transformer.resblocks.2.attn.in_proj_weight', 'transformer.resblocks.2.attn.in_proj_bias', 'transformer.resblocks.2.attn.out_proj.weight', 'transformer.resblocks.2.attn.out_proj.bias', 'transformer.resblocks.2.ln_1.weight', 'transformer.resblocks.2.ln_1.bias', 'transformer.resblocks.2.mlp.c_fc.weight', 'transformer.resblocks.2.mlp.c_fc.bias', 'transformer.resblocks.2.mlp.c_proj.weight', 'transformer.resblocks.2.mlp.c_proj.bias', 'transformer.resblocks.2.ln_2.weight', 'transformer.resblocks.2.ln_2.bias', 'transformer.resblocks.3.attn.in_proj_weight', 'transformer.resblocks.3.attn.in_proj_bias', 'transformer.resblocks.3.attn.out_proj.weight', 'transformer.resblocks.3.attn.out_proj.bias', 'transformer.resblocks.3.ln_1.weight', 'transformer.resblocks.3.ln_1.bias', 'transformer.resblocks.3.mlp.c_fc.weight', 'transformer.resblocks.3.mlp.c_fc.bias', 'transformer.resblocks.3.mlp.c_proj.weight', 'transformer.resblocks.3.mlp.c_proj.bias', 'transformer.resblocks.3.ln_2.weight', 'transformer.resblocks.3.ln_2.bias', 'transformer.resblocks.4.attn.in_proj_weight', 'transformer.resblocks.4.attn.in_proj_bias', 'transformer.resblocks.4.attn.out_proj.weight', 'transformer.resblocks.4.attn.out_proj.bias', 'transformer.resblocks.4.ln_1.weight', 'transformer.resblocks.4.ln_1.bias', 'transformer.resblocks.4.mlp.c_fc.weight', 'transformer.resblocks.4.mlp.c_fc.bias', 'transformer.resblocks.4.mlp.c_proj.weight', 'transformer.resblocks.4.mlp.c_proj.bias', 'transformer.resblocks.4.ln_2.weight', 'transformer.resblocks.4.ln_2.bias', 'transformer.resblocks.5.attn.in_proj_weight', 'transformer.resblocks.5.attn.in_proj_bias', 'transformer.resblocks.5.attn.out_proj.weight', 'transformer.resblocks.5.attn.out_proj.bias', 'transformer.resblocks.5.ln_1.weight', 'transformer.resblocks.5.ln_1.bias', 'transformer.resblocks.5.mlp.c_fc.weight', 'transformer.resblocks.5.mlp.c_fc.bias', 'transformer.resblocks.5.mlp.c_proj.weight', 'transformer.resblocks.5.mlp.c_proj.bias', 'transformer.resblocks.5.ln_2.weight', 'transformer.resblocks.5.ln_2.bias', 'transformer.resblocks.6.attn.in_proj_weight', 'transformer.resblocks.6.attn.in_proj_bias', 'transformer.resblocks.6.attn.out_proj.weight', 'transformer.resblocks.6.attn.out_proj.bias', 'transformer.resblocks.6.ln_1.weight', 'transformer.resblocks.6.ln_1.bias', 'transformer.resblocks.6.mlp.c_fc.weight', 'transformer.resblocks.6.mlp.c_fc.bias', 'transformer.resblocks.6.mlp.c_proj.weight', 'transformer.resblocks.6.mlp.c_proj.bias', 'transformer.resblocks.6.ln_2.weight', 'transformer.resblocks.6.ln_2.bias', 'transformer.resblocks.7.attn.in_proj_weight', 'transformer.resblocks.7.attn.in_proj_bias', 'transformer.resblocks.7.attn.out_proj.weight', 'transformer.resblocks.7.attn.out_proj.bias', 'transformer.resblocks.7.ln_1.weight', 'transformer.resblocks.7.ln_1.bias', 'transformer.resblocks.7.mlp.c_fc.weight', 'transformer.resblocks.7.mlp.c_fc.bias', 'transformer.resblocks.7.mlp.c_proj.weight', 'transformer.resblocks.7.mlp.c_proj.bias', 'transformer.resblocks.7.ln_2.weight', 'transformer.resblocks.7.ln_2.bias', 'transformer.resblocks.8.attn.in_proj_weight', 'transformer.resblocks.8.attn.in_proj_bias', 'transformer.resblocks.8.attn.out_proj.weight', 'transformer.resblocks.8.attn.out_proj.bias', 'transformer.resblocks.8.ln_1.weight', 'transformer.resblocks.8.ln_1.bias', 'transformer.resblocks.8.mlp.c_fc.weight', 'transformer.resblocks.8.mlp.c_fc.bias', 'transformer.resblocks.8.mlp.c_proj.weight', 'transformer.resblocks.8.mlp.c_proj.bias', 'transformer.resblocks.8.ln_2.weight', 'transformer.resblocks.8.ln_2.bias', 'transformer.resblocks.9.attn.in_proj_weight', 'transformer.resblocks.9.attn.in_proj_bias', 'transformer.resblocks.9.attn.out_proj.weight', 'transformer.resblocks.9.attn.out_proj.bias', 'transformer.resblocks.9.ln_1.weight', 'transformer.resblocks.9.ln_1.bias', 'transformer.resblocks.9.mlp.c_fc.weight', 'transformer.resblocks.9.mlp.c_fc.bias', 'transformer.resblocks.9.mlp.c_proj.weight', 'transformer.resblocks.9.mlp.c_proj.bias', 'transformer.resblocks.9.ln_2.weight', 'transformer.resblocks.9.ln_2.bias', 'transformer.resblocks.10.attn.in_proj_weight', 'transformer.resblocks.10.attn.in_proj_bias', 'transformer.resblocks.10.attn.out_proj.weight', 'transformer.resblocks.10.attn.out_proj.bias', 'transformer.resblocks.10.ln_1.weight', 'transformer.resblocks.10.ln_1.bias', 'transformer.resblocks.10.mlp.c_fc.weight', 'transformer.resblocks.10.mlp.c_fc.bias', 'transformer.resblocks.10.mlp.c_proj.weight', 'transformer.resblocks.10.mlp.c_proj.bias', 'transformer.resblocks.10.ln_2.weight', 'transformer.resblocks.10.ln_2.bias', 'transformer.resblocks.11.attn.in_proj_weight', 'transformer.resblocks.11.attn.in_proj_bias', 'transformer.resblocks.11.attn.out_proj.weight', 'transformer.resblocks.11.attn.out_proj.bias', 'transformer.resblocks.11.ln_1.weight', 'transformer.resblocks.11.ln_1.bias', 'transformer.resblocks.11.mlp.c_fc.weight', 'transformer.resblocks.11.mlp.c_fc.bias', 'transformer.resblocks.11.mlp.c_proj.weight', 'transformer.resblocks.11.mlp.c_proj.bias', 'transformer.resblocks.11.ln_2.weight', 'transformer.resblocks.11.ln_2.bias', 'ln_post.weight', 'ln_post.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_clip.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "module\n"
     ]
    }
   ],
   "source": [
    "model_key = 'model|module'\n",
    "\n",
    "for model_key in model_key.split('|'):\n",
    "    if model_key in checkpoint_clip:\n",
    "        checkpoint_model = checkpoint_clip[model_key]\n",
    "        print(\"Load state_dict by model_key = %s\" % model_key)\n",
    "        break\n",
    "    print(model_key)\n",
    "\n",
    "    \n",
    "#     if model_key in checkpoint:\n",
    "#         checkpoint_model = checkpoint[model_key]\n",
    "#         print(\"Load state_dict by model_key = %s\" % model_key)\n",
    "#         break\n",
    "# if checkpoint_model is None:\n",
    "#     checkpoint_model = checkpoint\n",
    "# state_dict = model.state_dict()\n",
    "# for k in ['head.weight', 'head.bias']:\n",
    "#     if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
    "#         print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "#         del checkpoint_model[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_all_keys = list(checkpoint_clip.keys())\n",
    "new_dict = OrderedDict()\n",
    "for key in clip_all_keys:\n",
    "        if key.startswith('transformer.'):\n",
    "            if key[23] == '.':\n",
    "                new_dict['blocks.'+ key[22] + '.clip_' + key[24:]] = checkpoint_clip[key]\n",
    "            else : # layer10 ~ 11 process\n",
    "                new_dict['blocks.'+ key[22:24] + '.clip_' + key[25:]] = checkpoint_clip[key]\n",
    "        else:\n",
    "            new_dict['clip_' + key] = checkpoint_clip[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_embedding', 'positional_embedding', 'proj', 'conv1.weight', 'ln_pre.weight', 'ln_pre.bias', 'transformer.resblocks.0.attn.in_proj_weight', 'transformer.resblocks.0.attn.in_proj_bias', 'transformer.resblocks.0.attn.out_proj.weight', 'transformer.resblocks.0.attn.out_proj.bias', 'transformer.resblocks.0.ln_1.weight', 'transformer.resblocks.0.ln_1.bias', 'transformer.resblocks.0.mlp.c_fc.weight', 'transformer.resblocks.0.mlp.c_fc.bias', 'transformer.resblocks.0.mlp.c_proj.weight', 'transformer.resblocks.0.mlp.c_proj.bias', 'transformer.resblocks.0.ln_2.weight', 'transformer.resblocks.0.ln_2.bias', 'transformer.resblocks.1.attn.in_proj_weight', 'transformer.resblocks.1.attn.in_proj_bias', 'transformer.resblocks.1.attn.out_proj.weight', 'transformer.resblocks.1.attn.out_proj.bias', 'transformer.resblocks.1.ln_1.weight', 'transformer.resblocks.1.ln_1.bias', 'transformer.resblocks.1.mlp.c_fc.weight', 'transformer.resblocks.1.mlp.c_fc.bias', 'transformer.resblocks.1.mlp.c_proj.weight', 'transformer.resblocks.1.mlp.c_proj.bias', 'transformer.resblocks.1.ln_2.weight', 'transformer.resblocks.1.ln_2.bias', 'transformer.resblocks.2.attn.in_proj_weight', 'transformer.resblocks.2.attn.in_proj_bias', 'transformer.resblocks.2.attn.out_proj.weight', 'transformer.resblocks.2.attn.out_proj.bias', 'transformer.resblocks.2.ln_1.weight', 'transformer.resblocks.2.ln_1.bias', 'transformer.resblocks.2.mlp.c_fc.weight', 'transformer.resblocks.2.mlp.c_fc.bias', 'transformer.resblocks.2.mlp.c_proj.weight', 'transformer.resblocks.2.mlp.c_proj.bias', 'transformer.resblocks.2.ln_2.weight', 'transformer.resblocks.2.ln_2.bias', 'transformer.resblocks.3.attn.in_proj_weight', 'transformer.resblocks.3.attn.in_proj_bias', 'transformer.resblocks.3.attn.out_proj.weight', 'transformer.resblocks.3.attn.out_proj.bias', 'transformer.resblocks.3.ln_1.weight', 'transformer.resblocks.3.ln_1.bias', 'transformer.resblocks.3.mlp.c_fc.weight', 'transformer.resblocks.3.mlp.c_fc.bias', 'transformer.resblocks.3.mlp.c_proj.weight', 'transformer.resblocks.3.mlp.c_proj.bias', 'transformer.resblocks.3.ln_2.weight', 'transformer.resblocks.3.ln_2.bias', 'transformer.resblocks.4.attn.in_proj_weight', 'transformer.resblocks.4.attn.in_proj_bias', 'transformer.resblocks.4.attn.out_proj.weight', 'transformer.resblocks.4.attn.out_proj.bias', 'transformer.resblocks.4.ln_1.weight', 'transformer.resblocks.4.ln_1.bias', 'transformer.resblocks.4.mlp.c_fc.weight', 'transformer.resblocks.4.mlp.c_fc.bias', 'transformer.resblocks.4.mlp.c_proj.weight', 'transformer.resblocks.4.mlp.c_proj.bias', 'transformer.resblocks.4.ln_2.weight', 'transformer.resblocks.4.ln_2.bias', 'transformer.resblocks.5.attn.in_proj_weight', 'transformer.resblocks.5.attn.in_proj_bias', 'transformer.resblocks.5.attn.out_proj.weight', 'transformer.resblocks.5.attn.out_proj.bias', 'transformer.resblocks.5.ln_1.weight', 'transformer.resblocks.5.ln_1.bias', 'transformer.resblocks.5.mlp.c_fc.weight', 'transformer.resblocks.5.mlp.c_fc.bias', 'transformer.resblocks.5.mlp.c_proj.weight', 'transformer.resblocks.5.mlp.c_proj.bias', 'transformer.resblocks.5.ln_2.weight', 'transformer.resblocks.5.ln_2.bias', 'transformer.resblocks.6.attn.in_proj_weight', 'transformer.resblocks.6.attn.in_proj_bias', 'transformer.resblocks.6.attn.out_proj.weight', 'transformer.resblocks.6.attn.out_proj.bias', 'transformer.resblocks.6.ln_1.weight', 'transformer.resblocks.6.ln_1.bias', 'transformer.resblocks.6.mlp.c_fc.weight', 'transformer.resblocks.6.mlp.c_fc.bias', 'transformer.resblocks.6.mlp.c_proj.weight', 'transformer.resblocks.6.mlp.c_proj.bias', 'transformer.resblocks.6.ln_2.weight', 'transformer.resblocks.6.ln_2.bias', 'transformer.resblocks.7.attn.in_proj_weight', 'transformer.resblocks.7.attn.in_proj_bias', 'transformer.resblocks.7.attn.out_proj.weight', 'transformer.resblocks.7.attn.out_proj.bias', 'transformer.resblocks.7.ln_1.weight', 'transformer.resblocks.7.ln_1.bias', 'transformer.resblocks.7.mlp.c_fc.weight', 'transformer.resblocks.7.mlp.c_fc.bias', 'transformer.resblocks.7.mlp.c_proj.weight', 'transformer.resblocks.7.mlp.c_proj.bias', 'transformer.resblocks.7.ln_2.weight', 'transformer.resblocks.7.ln_2.bias', 'transformer.resblocks.8.attn.in_proj_weight', 'transformer.resblocks.8.attn.in_proj_bias', 'transformer.resblocks.8.attn.out_proj.weight', 'transformer.resblocks.8.attn.out_proj.bias', 'transformer.resblocks.8.ln_1.weight', 'transformer.resblocks.8.ln_1.bias', 'transformer.resblocks.8.mlp.c_fc.weight', 'transformer.resblocks.8.mlp.c_fc.bias', 'transformer.resblocks.8.mlp.c_proj.weight', 'transformer.resblocks.8.mlp.c_proj.bias', 'transformer.resblocks.8.ln_2.weight', 'transformer.resblocks.8.ln_2.bias', 'transformer.resblocks.9.attn.in_proj_weight', 'transformer.resblocks.9.attn.in_proj_bias', 'transformer.resblocks.9.attn.out_proj.weight', 'transformer.resblocks.9.attn.out_proj.bias', 'transformer.resblocks.9.ln_1.weight', 'transformer.resblocks.9.ln_1.bias', 'transformer.resblocks.9.mlp.c_fc.weight', 'transformer.resblocks.9.mlp.c_fc.bias', 'transformer.resblocks.9.mlp.c_proj.weight', 'transformer.resblocks.9.mlp.c_proj.bias', 'transformer.resblocks.9.ln_2.weight', 'transformer.resblocks.9.ln_2.bias', 'transformer.resblocks.10.attn.in_proj_weight', 'transformer.resblocks.10.attn.in_proj_bias', 'transformer.resblocks.10.attn.out_proj.weight', 'transformer.resblocks.10.attn.out_proj.bias', 'transformer.resblocks.10.ln_1.weight', 'transformer.resblocks.10.ln_1.bias', 'transformer.resblocks.10.mlp.c_fc.weight', 'transformer.resblocks.10.mlp.c_fc.bias', 'transformer.resblocks.10.mlp.c_proj.weight', 'transformer.resblocks.10.mlp.c_proj.bias', 'transformer.resblocks.10.ln_2.weight', 'transformer.resblocks.10.ln_2.bias', 'transformer.resblocks.11.attn.in_proj_weight', 'transformer.resblocks.11.attn.in_proj_bias', 'transformer.resblocks.11.attn.out_proj.weight', 'transformer.resblocks.11.attn.out_proj.bias', 'transformer.resblocks.11.ln_1.weight', 'transformer.resblocks.11.ln_1.bias', 'transformer.resblocks.11.mlp.c_fc.weight', 'transformer.resblocks.11.mlp.c_fc.bias', 'transformer.resblocks.11.mlp.c_proj.weight', 'transformer.resblocks.11.mlp.c_proj.bias', 'transformer.resblocks.11.ln_2.weight', 'transformer.resblocks.11.ln_2.bias', 'ln_post.weight', 'ln_post.bias']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
